{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dded4de7",
   "metadata": {},
   "source": [
    "# APRENDIZAJE PROFUNDO Y SERIES TEMPORALES\n",
    "\n",
    "## por: Marco Cajamarca, Adrian Campoverde y Pablo Bravo\n",
    "\n",
    "A continuaci√≥n, se desarrollar√° una funci√≥n que utiliza una para predecir la demanda de productos a partir de datos hist√≥ricos reales. Este enfoque permitir√° anticipar las necesidades de inventario y tomar decisiones informadas basadas en el comportamiento temporal de las ventas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de83e1ef",
   "metadata": {},
   "source": [
    "`source ~/tf-env/bin/activate`\n",
    "`mlflow ui --backend-store-uri file:mlruns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a01180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Embedding, Concatenate\n",
    "from IPython.display import display\n",
    "import mlflow\n",
    "import joblib\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95a571f",
   "metadata": {},
   "source": [
    "# Configuracion de mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a88c655",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()\n",
    "mlflow_tracking_dir = os.path.join(notebook_dir, 'mlruns')\n",
    "\n",
    "os.makedirs(mlflow_tracking_dir, exist_ok=True)\n",
    "\n",
    "mlflow.set_tracking_uri(f'file://{mlflow_tracking_dir}')\n",
    "\n",
    "experiment_name = 'stock_demand_forecasting'\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Obtener informaci√≥n del experimento\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "print(f\"Experimento: {experiment_name}\")\n",
    "print(f\"Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"Directorio MLflow: {mlflow_tracking_dir}\")\n",
    "print(f\"Experiment ID: {experiment.experiment_id}\")\n",
    "\n",
    "mlflow.keras.autolog(log_models=True, log_datasets=True, disable=False, exclusive=False)\n",
    "\n",
    "print(\"\\n‚úÖ MLflow configurado correctamente\")\n",
    "print(\"üí° Para ver la UI, ejecuta en terminal: mlflow ui --backend-store-uri file://./mlruns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943d4196",
   "metadata": {},
   "source": [
    "### configuracion b√°sica de tensorflow para uso optimo de gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5378528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for g in gpus:\n",
    "            tf.config.experimental.set_memory_growth(g, True)\n",
    "    except Exception as e:\n",
    "        print(\"GPU config error:\", e)\n",
    "\n",
    "# Habilitar XLA JIT para acelerar operaciones\n",
    "tf.config.optimizer.set_jit(True)\n",
    "\n",
    "# Ajustar hilos para aprovechar CPU en preprocesamiento\n",
    "ncpu = os.cpu_count() or 4\n",
    "tf.config.threading.set_intra_op_parallelism_threads(ncpu)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(ncpu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691be7b8",
   "metadata": {},
   "source": [
    "# Fase 1:\n",
    "An√°lisis y Preparaci√≥n del Dataset\n",
    "\n",
    "Objetivo:\n",
    "Cargar, explorar y preparar el dataset para el entrenamiento de modelos de aprendizaje profundo.\n",
    "En esta fase se realiza una inspecci√≥n inicial de los datos, identificaci√≥n de valores faltantes, an√°lisis de las variables y creaci√≥n de caracter√≠sticas temporales relevantes para el modelado de series temporales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd67fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el archivo CSV real\n",
    "ruta = Path('practica_completo.csv')\n",
    "df = pd.read_csv(ruta)\n",
    "\n",
    "# Mostrar columnas y primeras filas\n",
    "print(\"Columnas:\", df.columns.tolist())\n",
    "display(df.head())\n",
    "\n",
    "# Tipos de datos y valores faltantes\n",
    "print(\"\\nTipos de datos:\")\n",
    "display(df.dtypes)\n",
    "print(\"\\nValores faltantes por columna:\")\n",
    "display(df.isnull().sum())\n",
    "\n",
    "# Mostrar rango de fechas del dataset\n",
    "print(\"Primer registro:\", df['created_at'].min())\n",
    "print(\"√öltimo registro:\", df['created_at'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f8e35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar filas con valores nulos cr√≠ticos\n",
    "df = df.dropna(subset=['created_at', 'product_id', 'salida'])\n",
    "\n",
    "# Eliminar duplicados si existen\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Convertir fechas a datetime\n",
    "df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "df['last_order_date'] = pd.to_datetime(df['last_order_date'], errors='coerce')\n",
    "\n",
    "# Revisar valores extremos en ventas\n",
    "print(\"Ventas (salida) - resumen estad√≠stico:\")\n",
    "display(df['salida'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9da7718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear variables temporales\n",
    "df['dia_semana'] = df['created_at'].dt.dayofweek\n",
    "df['mes'] = df['created_at'].dt.month\n",
    "df['fin_semana'] = df['dia_semana'].isin([5,6]).astype(int)\n",
    "\n",
    "# Feriados (ajusta seg√∫n tu pa√≠s)\n",
    "feriados = [\n",
    "    '2024-01-01', '2024-02-12', '2024-02-13', '2024-03-29', '2024-05-01',\n",
    "    '2024-05-24', '2024-08-10', '2024-10-09', '2024-11-02', '2024-11-03', '2024-12-25'\n",
    "]\n",
    "df['fecha'] = df['created_at'].dt.date.astype(str)\n",
    "df['feriado'] = df['fecha'].isin(feriados).astype(int)\n",
    "\n",
    "# Antig√ºedad del producto (en d√≠as)\n",
    "df['antiguedad_producto'] = (df['created_at'] - df.groupby('product_id')['created_at'].transform('min')).dt.days\n",
    "\n",
    "# Ratio vendida/stock (si aplica)\n",
    "df['ratio_vendida_stock'] = df['salida'] / (df['quantity_on_hand'] + 1e-6)\n",
    "\n",
    "# Mostrar ejemplo\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50402e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables a usar en la secuencia\n",
    "features = ['salida', 'dia_semana', 'mes', 'fin_semana', 'feriado', 'quantity_on_hand', 'unit_cost']\n",
    "ventana = 7  # d√≠as de historial\n",
    "\n",
    "def crear_ventanas_con_producto(df, features, ventana=7):\n",
    "    \"\"\"Crea secuencias manteniendo el product_id y prediciendo stock\"\"\"\n",
    "    X_seq, y_stock, product_ids, fechas = [], [], [], []\n",
    "    \n",
    "    for product_id, grupo in df.groupby('product_id'):\n",
    "        grupo = grupo.sort_values('created_at')\n",
    "        datos = grupo[features].values\n",
    "        stock = grupo['quantity_on_hand'].values\n",
    "        fechas_grupo = grupo['created_at'].values\n",
    "        \n",
    "        for i in range(len(datos) - ventana):\n",
    "            X_seq.append(datos[i:i+ventana])\n",
    "            y_stock.append(stock[i+ventana])\n",
    "            product_ids.append(product_id)\n",
    "            fechas.append(fechas_grupo[i+ventana])\n",
    "    \n",
    "    return np.array(X_seq), np.array(y_stock), np.array(product_ids), np.array(fechas)\n",
    "\n",
    "X_seq, y_seq, product_ids, fechas = crear_ventanas_con_producto(df, features, ventana=ventana)\n",
    "print(\"Forma de X_seq:\", X_seq.shape)\n",
    "print(\"Forma de y_seq (stock):\", y_seq.shape)\n",
    "print(\"Productos √∫nicos:\", len(np.unique(product_ids)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6688a4",
   "metadata": {},
   "source": [
    "# Fase 2:\n",
    "## Desarrollo y Entrenamiento del Modelo\n",
    "\n",
    "### Objetivo:\n",
    "Implementar un modelo de Deep Learning usando RNN (por ejemplo, LSTM o GRU) para predecir la demanda futura."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6090bf",
   "metadata": {},
   "source": [
    "Limpieza y normalizacion de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491bb209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "from scipy import stats\n",
    "\n",
    "n_samples, ventana, n_features = X_seq.shape\n",
    "\n",
    "# Filtrar productos con stock muy bajo o cero\n",
    "print(f\"üìä Filtrando datos con stock bajo...\")\n",
    "print(f\"   Muestras originales: {len(y_seq)}\")\n",
    "mask_stock_valido = y_seq >= 10  # Solo stock >= 10 unidades\n",
    "X_seq_filtrado = X_seq[mask_stock_valido]\n",
    "y_seq_filtrado = y_seq[mask_stock_valido]\n",
    "product_ids_filtrado = product_ids[mask_stock_valido]\n",
    "print(f\"   Muestras despu√©s de filtrar (stock >= 10): {len(y_seq_filtrado)}\")\n",
    "print(f\"   Removidos: {len(y_seq) - len(y_seq_filtrado)} ({(len(y_seq) - len(y_seq_filtrado))/len(y_seq)*100:.2f}%)\")\n",
    "\n",
    "# Transformaci√≥n logar√≠tmica del target para mejorar distribuci√≥n\n",
    "y_seq_log = np.log1p(y_seq_filtrado)  # log(1 + x) para evitar log(0)\n",
    "\n",
    "# Codificar product_ids\n",
    "le = LabelEncoder()\n",
    "product_ids_encoded = le.fit_transform(product_ids_filtrado)\n",
    "joblib.dump(le, 'product_encoder.joblib')\n",
    "\n",
    "# Normalizar secuencias\n",
    "scaler = RobustScaler()\n",
    "X_seq_reshaped = X_seq_filtrado.reshape(-1, n_features)\n",
    "X_seq_scaled = scaler.fit_transform(np.nan_to_num(X_seq_reshaped, nan=0))\n",
    "X_seq_scaled = X_seq_scaled.reshape(-1, ventana, n_features)\n",
    "\n",
    "# Split inicial con target logar√≠tmico\n",
    "X_train_raw, X_val, y_train_log_raw, y_val_log, pid_train_raw, pid_val = train_test_split(\n",
    "    X_seq_scaled, y_seq_log, product_ids_encoded, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Eliminar outliers en escala logar√≠tmica (m√°s suave)\n",
    "z_scores = np.abs(stats.zscore(y_train_log_raw))\n",
    "mask_no_outliers = z_scores < 3.5\n",
    "X_train = X_train_raw[mask_no_outliers]\n",
    "y_train_log = y_train_log_raw[mask_no_outliers]\n",
    "pid_train = pid_train_raw[mask_no_outliers]\n",
    "\n",
    "outliers_removed = len(y_train_log_raw) - len(y_train_log)\n",
    "print(f\"\\nüìä Limpieza de outliers:\")\n",
    "print(f\"   Outliers removidos: {outliers_removed} ({outliers_removed/len(y_train_log_raw)*100:.2f}%)\")\n",
    "print(f\"   Train: {len(y_train_log)}, Val: {len(y_val_log)}\")\n",
    "print(f\"   Stock (escala log) - Min: {y_train_log.min():.3f}, Max: {y_train_log.max():.3f}, Media: {y_train_log.mean():.3f}\")\n",
    "print(f\"   Stock (escala real) - Min: {np.expm1(y_train_log).min():.1f}, Max: {np.expm1(y_train_log).max():.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7324e1c9",
   "metadata": {},
   "source": [
    "# Creaci√≥n del modelo con embedding de productos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8609446f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_productos = len(df['product_id'].unique())\n",
    "embedding_dim = 32  # Baseline dimension\n",
    "\n",
    "# Input de secuencia temporal\n",
    "seq_input = keras.layers.Input(shape=(X_train.shape[1], X_train.shape[2]), name='sequence')\n",
    "product_input = keras.layers.Input(shape=(1,), name='product_id')\n",
    "\n",
    "# Embedding del producto\n",
    "product_embedding = Embedding(\n",
    "    input_dim=n_productos + 1,\n",
    "    output_dim=embedding_dim,\n",
    "    name='product_embedding'\n",
    ")(product_input)\n",
    "product_embedding = keras.layers.Flatten()(product_embedding)\n",
    "\n",
    "# Procesamiento de secuencia temporal - arquitectura baseline simplificada\n",
    "x = keras.layers.Bidirectional(keras.layers.GRU(\n",
    "    128, \n",
    "    return_sequences=True,\n",
    "    kernel_regularizer=keras.regularizers.l2(0.0001)\n",
    "))(seq_input)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "x = keras.layers.Bidirectional(keras.layers.GRU(\n",
    "    64, \n",
    "    return_sequences=False,\n",
    "    kernel_regularizer=keras.regularizers.l2(0.0001)\n",
    "))(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "# Combinar informaci√≥n de producto + secuencia temporal\n",
    "combined = Concatenate()([x, product_embedding])\n",
    "\n",
    "# Capas densas\n",
    "x = keras.layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.0001))(combined)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "# Output en escala logar√≠tmica\n",
    "output = keras.layers.Dense(1, name='stock_log_predicho')(x)\n",
    "\n",
    "model = keras.Model(inputs=[seq_input, product_input], outputs=output)\n",
    "\n",
    "# Custom weighted loss que da m√°s importancia a valores altos de stock\n",
    "@tf.function\n",
    "def weighted_huber_loss(y_true, y_pred, delta=1.0, high_stock_threshold=5.3):\n",
    "    \"\"\"\n",
    "    Huber loss con pesos adaptativos basados en el valor del stock (en escala log)\n",
    "    high_stock_threshold en escala log: log1p(200) ‚âà 5.3\n",
    "    \"\"\"\n",
    "    # Calcular error\n",
    "    error = y_true - y_pred\n",
    "    abs_error = tf.abs(error)\n",
    "    \n",
    "    # Huber loss b√°sico\n",
    "    quadratic = tf.minimum(abs_error, delta)\n",
    "    linear = abs_error - quadratic\n",
    "    loss = 0.5 * quadratic**2 + delta * linear\n",
    "    \n",
    "    # Calcular pesos adaptativos: mayor peso para stock alto\n",
    "    # Stock alto (log >= 5.3): peso 2.5x\n",
    "    # Stock medio (log 3.9-5.3): peso 1.5x  \n",
    "    # Stock bajo (log < 3.9): peso 1.0x\n",
    "    weights = tf.where(\n",
    "        y_true >= high_stock_threshold,\n",
    "        2.5,  # Stock >= 200 unidades (log1p(200) ‚âà 5.3)\n",
    "        tf.where(\n",
    "            y_true >= 3.9,  # Stock >= 50 unidades (log1p(50) ‚âà 3.9)\n",
    "            1.5,\n",
    "            1.0\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Aplicar pesos a la loss\n",
    "    weighted_loss = loss * weights\n",
    "    \n",
    "    return tf.reduce_mean(weighted_loss)\n",
    "\n",
    "# Learning rate schedule m√°s conservador\n",
    "initial_learning_rate = 0.001\n",
    "lr_schedule = keras.optimizers.schedules.CosineDecayRestarts(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    first_decay_steps=100,\n",
    "    t_mul=2.0,\n",
    "    m_mul=0.9,\n",
    "    alpha=0.0001\n",
    ")\n",
    "\n",
    "optimizer = keras.optimizers.Adam(\n",
    "    learning_rate=lr_schedule,\n",
    "    clipnorm=1.0\n",
    ")\n",
    "\n",
    "# Compilar con loss personalizada\n",
    "model.compile(\n",
    "    optimizer=optimizer, \n",
    "    loss=weighted_huber_loss,\n",
    "    metrics=['mae', keras.metrics.RootMeanSquaredError(name='rmse')]\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=25,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1,\n",
    "    min_delta=1e-5,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    'best_model_v2.keras',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(\n",
    "    log_dir='./logs',\n",
    "    histogram_freq=1,\n",
    "    write_graph=True,\n",
    "    update_freq='epoch'\n",
    ")\n",
    "\n",
    "run_params = {\n",
    "    'ventana': ventana,\n",
    "    'n_features': n_features,\n",
    "    'n_productos': n_productos,\n",
    "    'embedding_dim': embedding_dim,\n",
    "    'architecture': 'Baseline_BiGRU_WeightedLoss',\n",
    "    'total_params': model.count_params(),\n",
    "    'optimizer': 'adam_with_cosine_decay',\n",
    "    'initial_learning_rate': initial_learning_rate,\n",
    "    'loss_function': 'weighted_huber',\n",
    "    'loss_weights': 'high_stock_2.5x_medium_1.5x_low_1.0x',\n",
    "    'target_transform': 'log1p',\n",
    "    'output_activation': 'linear',\n",
    "    'scaler': 'RobustScaler',\n",
    "    'outlier_removal': 'z_score_3.5',\n",
    "    'min_stock_threshold': 10,\n",
    "    'outliers_removed': outliers_removed,\n",
    "    'batch_size': 256,\n",
    "    'epochs': 150,\n",
    "    'prediction_target': 'quantity_on_hand_log',\n",
    "    'enhancements': 'weighted_loss_for_high_stock+batch_norm'\n",
    "}\n",
    "\n",
    "with mlflow.start_run(run_name=f'stock_weighted_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}') as run:\n",
    "    mlflow.log_params(run_params)\n",
    "    mlflow.log_param('train_samples', len(X_train))\n",
    "    mlflow.log_param('val_samples', len(X_val))\n",
    "    \n",
    "    model_summary = []\n",
    "    model.summary(print_fn=lambda x: model_summary.append(x))\n",
    "    model_summary_str = '\\n'.join(model_summary)\n",
    "    \n",
    "    with open('model_summary.txt', 'w') as f:\n",
    "        f.write(model_summary_str)\n",
    "    mlflow.log_artifact('model_summary.txt')\n",
    "    \n",
    "    print(f\"\\nRun ID: {run.info.run_id}\")\n",
    "    print(f\"Estrategia: Weighted Loss - Stock alto (>=200) peso 2.5x, medio (50-200) peso 1.5x\")\n",
    "    \n",
    "    history = model.fit(\n",
    "        [X_train, pid_train],\n",
    "        y_train_log,\n",
    "        validation_data=([X_val, pid_val], y_val_log),\n",
    "        epochs=run_params['epochs'],\n",
    "        batch_size=run_params['batch_size'],\n",
    "        callbacks=[early_stopping, model_checkpoint, tensorboard],\n",
    "        verbose=1,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    scaler_path = 'scaler.joblib'\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    mlflow.log_artifact(scaler_path)\n",
    "    mlflow.log_artifact('product_encoder.joblib')\n",
    "    \n",
    "    if os.path.exists('best_model_v2.keras'):\n",
    "        mlflow.log_artifact('best_model_v2.keras')\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    axes[0, 0].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "    axes[0, 0].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "    axes[0, 0].set_title('Model Loss (Weighted Huber en escala log)')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[0, 1].plot(history.history['mae'], label='Train MAE', linewidth=2)\n",
    "    axes[0, 1].plot(history.history['val_mae'], label='Val MAE', linewidth=2)\n",
    "    axes[0, 1].set_title('Mean Absolute Error (escala log)')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('MAE')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, 0].plot(history.history['rmse'], label='Train RMSE', linewidth=2)\n",
    "    axes[1, 0].plot(history.history['val_rmse'], label='Val RMSE', linewidth=2)\n",
    "    axes[1, 0].set_title('RMSE (escala log)')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('RMSE')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, 1].text(0.5, 0.5, \n",
    "                   f'GRU Bidireccional Baseline\\n{n_productos} productos\\nEmbedding: {embedding_dim}D\\nTarget: log1p(stock)\\nWeighted Huber Loss\\nAlto: 2.5x | Medio: 1.5x | Bajo: 1.0x', \n",
    "                   ha='center', va='center', transform=axes[1, 1].transAxes, fontsize=10)\n",
    "    axes[1, 1].set_title('Configuraci√≥n del Modelo')\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png', dpi=150, bbox_inches='tight')\n",
    "    mlflow.log_artifact('training_history.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f53ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el modelo\n",
    "print(\"üî¨ Evaluando modelo en conjunto de validaci√≥n...\")\n",
    "y_pred_log = model.predict([X_val, pid_val], batch_size=256)\n",
    "y_pred_log = np.asarray(y_pred_log).ravel().astype(np.float32)\n",
    "\n",
    "# Convertir de escala logar√≠tmica a escala real\n",
    "y_pred = np.expm1(y_pred_log)  # Inversa de log1p\n",
    "y_val_real = np.expm1(y_val_log)\n",
    "\n",
    "# Asegurar que no hay predicciones negativas\n",
    "y_pred = np.maximum(y_pred, 0)\n",
    "\n",
    "# Calcular m√©tricas en escala real\n",
    "rmse = np.sqrt(mean_squared_error(y_val_real, y_pred))\n",
    "mae = mean_absolute_error(y_val_real, y_pred)\n",
    "median_ae = np.median(np.abs(y_val_real - y_pred))\n",
    "\n",
    "# MAPE solo para stock >= 10\n",
    "threshold = 10\n",
    "mask_mape = y_val_real >= threshold\n",
    "if np.sum(mask_mape) > 0:\n",
    "    mape = np.mean(np.abs((y_val_real[mask_mape] - y_pred[mask_mape]) / y_val_real[mask_mape])) * 100\n",
    "else:\n",
    "    mape = 0.0\n",
    "\n",
    "# R¬≤\n",
    "r2 = 1 - (np.sum((y_val_real - y_pred)**2) / np.sum((y_val_real - y_val_real.mean())**2))\n",
    "\n",
    "# M√©tricas por rangos de stock\n",
    "mask_bajo = (y_val_real >= 10) & (y_val_real < 50)\n",
    "mask_medio = (y_val_real >= 50) & (y_val_real < 200)\n",
    "mask_alto = y_val_real >= 200\n",
    "\n",
    "print(f\"\\nüìä M√©tricas Finales (Predicci√≥n de Stock):\")\n",
    "print(f\"  RMSE: {rmse:.2f}\")\n",
    "print(f\"  MAE: {mae:.2f}\")\n",
    "print(f\"  Median AE: {median_ae:.2f}\")\n",
    "print(f\"  MAPE (stock >= {threshold}): {mape:.2f}%\")\n",
    "print(f\"  R¬≤: {r2:.4f}\")\n",
    "print(f\"\\nüìà Precisi√≥n por rango:\")\n",
    "if np.sum(mask_bajo) > 0:\n",
    "    mape_bajo = np.mean(np.abs((y_val_real[mask_bajo] - y_pred[mask_bajo]) / y_val_real[mask_bajo])) * 100\n",
    "    print(f\"  Stock bajo (10-50): MAPE={mape_bajo:.2f}% | N={np.sum(mask_bajo)}\")\n",
    "if np.sum(mask_medio) > 0:\n",
    "    mape_medio = np.mean(np.abs((y_val_real[mask_medio] - y_pred[mask_medio]) / y_val_real[mask_medio])) * 100\n",
    "    print(f\"  Stock medio (50-200): MAPE={mape_medio:.2f}% | N={np.sum(mask_medio)}\")\n",
    "if np.sum(mask_alto) > 0:\n",
    "    mape_alto = np.mean(np.abs((y_val_real[mask_alto] - y_pred[mask_alto]) / y_val_real[mask_alto])) * 100\n",
    "    print(f\"  Stock alto (>=200): MAPE={mape_alto:.2f}% | N={np.sum(mask_alto)}\")\n",
    "\n",
    "print(f\"\\nüîç An√°lisis de predicciones:\")\n",
    "print(f\"  Rango predicciones: [{y_pred.min():.1f}, {y_pred.max():.1f}]\")\n",
    "print(f\"  Rango reales: [{y_val_real.min():.1f}, {y_val_real.max():.1f}]\")\n",
    "print(f\"  Media predicciones: {y_pred.mean():.1f}\")\n",
    "print(f\"  Media reales: {y_val_real.mean():.1f}\")\n",
    "\n",
    "try:\n",
    "    with mlflow.start_run(run_id=run.info.run_id):\n",
    "        mlflow.log_metric('final_rmse', float(rmse))\n",
    "        mlflow.log_metric('final_mae', float(mae))\n",
    "        mlflow.log_metric('final_median_ae', float(median_ae))\n",
    "        mlflow.log_metric('final_mape', float(mape))\n",
    "        mlflow.log_metric('final_r2', float(r2))\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "        \n",
    "        # Predicci√≥n vs Real\n",
    "        axes[0, 0].plot(y_val_real[:100], 'o-', label='Real', alpha=0.7, markersize=4)\n",
    "        axes[0, 0].plot(y_pred[:100], 's-', label='Predicci√≥n', alpha=0.7, markersize=3)\n",
    "        axes[0, 0].set_title('Predicci√≥n vs Valor Real (Stock) - Primeros 100')\n",
    "        axes[0, 0].set_xlabel('Ejemplo')\n",
    "        axes[0, 0].set_ylabel('Stock (unidades)')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Scatter plot\n",
    "        axes[0, 1].scatter(y_val_real, y_pred, alpha=0.3, s=10)\n",
    "        axes[0, 1].plot([y_val_real.min(), y_val_real.max()], [y_val_real.min(), y_val_real.max()], \n",
    "                     'r--', lw=2, label='Predicci√≥n Perfecta')\n",
    "        axes[0, 1].set_xlabel('Stock Real')\n",
    "        axes[0, 1].set_ylabel('Stock Predicho')\n",
    "        axes[0, 1].set_title(f'Correlaci√≥n (R¬≤={r2:.3f}, MAPE={mape:.1f}%)')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Distribuci√≥n de residuos\n",
    "        residuos = y_val_real - y_pred\n",
    "        axes[1, 0].hist(residuos, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "        axes[1, 0].axvline(x=0, color='r', linestyle='--', linewidth=2, label='Error = 0')\n",
    "        axes[1, 0].axvline(x=np.median(residuos), color='green', linestyle='--', linewidth=2, \n",
    "                          label=f'Mediana={np.median(residuos):.1f}')\n",
    "        axes[1, 0].set_title(f'Distribuci√≥n de Residuos (Œº={residuos.mean():.1f}, œÉ={residuos.std():.1f})')\n",
    "        axes[1, 0].set_xlabel('Residuo (Real - Predicci√≥n)')\n",
    "        axes[1, 0].set_ylabel('Frecuencia')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Error porcentual absoluto\n",
    "        error_pct = np.abs((y_val_real - y_pred) / (y_val_real + 1)) * 100\n",
    "        error_pct_filtrado = error_pct[error_pct < 100]\n",
    "        axes[1, 1].hist(error_pct_filtrado, bins=50, edgecolor='black', alpha=0.7, color='coral')\n",
    "        axes[1, 1].axvline(x=np.median(error_pct_filtrado), color='darkred', \n",
    "                          linestyle='--', linewidth=2, label=f'Mediana={np.median(error_pct_filtrado):.1f}%')\n",
    "        axes[1, 1].axvline(x=np.mean(error_pct_filtrado), color='darkblue', \n",
    "                          linestyle='--', linewidth=2, label=f'Media={np.mean(error_pct_filtrado):.1f}%')\n",
    "        axes[1, 1].set_title(f'Error Porcentual Absoluto (< 100%)')\n",
    "        axes[1, 1].set_xlabel('Error %')\n",
    "        axes[1, 1].set_ylabel('Frecuencia')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('prediction_analysis.png', dpi=150, bbox_inches='tight')\n",
    "        mlflow.log_artifact('prediction_analysis.png')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\n‚úÖ M√©tricas guardadas en MLflow (Run ID: {run.info.run_id})\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f'‚ö†Ô∏è Error al registrar m√©tricas: {e}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f0602f",
   "metadata": {},
   "source": [
    "# Funci√≥n de Predicci√≥n para Uso en Producci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10b18ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def predecir_stock_futuro(model, scaler, label_encoder, df, product_id, fecha_objetivo, ventana=7):\n",
    "    \"\"\"\n",
    "    Predice el stock para un producto en una fecha espec√≠fica\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado (predice en escala log)\n",
    "        scaler: RobustScaler ajustado\n",
    "        label_encoder: LabelEncoder para product_ids\n",
    "        df: DataFrame con datos hist√≥ricos\n",
    "        product_id: ID del producto\n",
    "        fecha_objetivo: Fecha para la predicci√≥n (datetime o string)\n",
    "        ventana: D√≠as de historial necesarios (default: 7)\n",
    "    \n",
    "    Returns:\n",
    "        dict con product_id, fecha, stock_predicho, stock_actual\n",
    "    \"\"\"\n",
    "    if isinstance(fecha_objetivo, str):\n",
    "        fecha_objetivo = pd.to_datetime(fecha_objetivo)\n",
    "    \n",
    "    # Filtrar datos del producto\n",
    "    df_producto = df[df['product_id'] == product_id].sort_values('created_at')\n",
    "    \n",
    "    if len(df_producto) == 0:\n",
    "        raise ValueError(f\"Producto {product_id} no encontrado\")\n",
    "    \n",
    "    # Obtener √∫ltimos 'ventana' d√≠as antes de la fecha objetivo\n",
    "    datos_recientes = df_producto[df_producto['created_at'] < fecha_objetivo].tail(ventana)\n",
    "    \n",
    "    if len(datos_recientes) < ventana:\n",
    "        raise ValueError(f\"No hay suficiente historial para producto {product_id} (necesita {ventana} d√≠as)\")\n",
    "    \n",
    "    # Preparar input de secuencia\n",
    "    X_input = datos_recientes[features].values.reshape(1, ventana, -1)\n",
    "    X_scaled = scaler.transform(X_input.reshape(-1, len(features))).reshape(1, ventana, -1)\n",
    "    \n",
    "    # Preparar input de product_id\n",
    "    try:\n",
    "        product_id_encoded = label_encoder.transform([product_id])[0]\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"Producto {product_id} no fue visto durante el entrenamiento\")\n",
    "    \n",
    "    product_id_array = np.array([[product_id_encoded]])\n",
    "    \n",
    "    # Predecir en escala logar√≠tmica y convertir a escala real\n",
    "    stock_log_predicho = model.predict([X_scaled, product_id_array], verbose=0)[0][0]\n",
    "    stock_predicho = np.expm1(stock_log_predicho)  # Inversa de log1p\n",
    "    \n",
    "    return {\n",
    "        'product_id': product_id,\n",
    "        'fecha': fecha_objetivo,\n",
    "        'stock_predicho': max(0, float(stock_predicho)),\n",
    "        'stock_actual': float(datos_recientes['quantity_on_hand'].iloc[-1]),\n",
    "        'ultima_fecha_datos': datos_recientes['created_at'].iloc[-1]\n",
    "    }\n",
    "\n",
    "# Ejemplo de uso\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EJEMPLO DE PREDICCI√ìN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Seleccionar un producto aleatorio\n",
    "producto_ejemplo = df['product_id'].value_counts().index[0]\n",
    "fecha_futura = datetime.now() + timedelta(days=3)\n",
    "\n",
    "try:\n",
    "    resultado = predecir_stock_futuro(\n",
    "        model=model,\n",
    "        scaler=scaler,\n",
    "        label_encoder=le,\n",
    "        df=df,\n",
    "        product_id=producto_ejemplo,\n",
    "        fecha_objetivo=fecha_futura,\n",
    "        ventana=ventana\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüì¶ Producto ID: {resultado['product_id']}\")\n",
    "    print(f\"üìÖ Fecha de predicci√≥n: {resultado['fecha'].strftime('%Y-%m-%d')}\")\n",
    "    print(f\"üìä Stock actual (√∫ltimo dato): {resultado['stock_actual']:.0f} unidades\")\n",
    "    print(f\"üîÆ Stock predicho: {resultado['stock_predicho']:.0f} unidades\")\n",
    "    print(f\"üìå √öltima fecha con datos: {resultado['ultima_fecha_datos'].strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    diferencia = resultado['stock_predicho'] - resultado['stock_actual']\n",
    "    if diferencia > 0:\n",
    "        print(f\"üìà Tendencia: +{diferencia:.0f} unidades\")\n",
    "    else:\n",
    "        print(f\"üìâ Tendencia: {diferencia:.0f} unidades\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80336bac",
   "metadata": {},
   "source": [
    "# Predicci√≥n para M√∫ltiples Productos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bed1f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predecir_stock_multiple(model, scaler, label_encoder, df, product_ids, fecha_objetivo, ventana=7):\n",
    "    \"\"\"\n",
    "    Predice stock para m√∫ltiples productos\n",
    "    \n",
    "    Args:\n",
    "        product_ids: Lista de product_ids\n",
    "        fecha_objetivo: Fecha para predicci√≥n\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame con predicciones\n",
    "    \"\"\"\n",
    "    resultados = []\n",
    "    \n",
    "    for pid in product_ids:\n",
    "        try:\n",
    "            res = predecir_stock_futuro(model, scaler, label_encoder, df, pid, fecha_objetivo, ventana)\n",
    "            resultados.append(res)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error con producto {pid}: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(resultados)\n",
    "\n",
    "# Ejemplo: predecir para los 5 productos m√°s vendidos\n",
    "top_productos = df['product_id'].value_counts().head(5).index.tolist()\n",
    "fecha_prediccion = datetime.now() + timedelta(days=7)\n",
    "\n",
    "print(f\"\\nüîÆ Predicci√≥n de stock para {len(top_productos)} productos\")\n",
    "print(f\"üìÖ Fecha objetivo: {fecha_prediccion.strftime('%Y-%m-%d')}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "predicciones_df = predecir_stock_multiple(\n",
    "    model=model,\n",
    "    scaler=scaler,\n",
    "    label_encoder=le,\n",
    "    df=df,\n",
    "    product_ids=top_productos,\n",
    "    fecha_objetivo=fecha_prediccion,\n",
    "    ventana=7\n",
    ")\n",
    "\n",
    "display(predicciones_df)\n",
    "\n",
    "# Visualizaci√≥n\n",
    "if len(predicciones_df) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    x = np.arange(len(predicciones_df))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax.bar(x - width/2, predicciones_df['stock_actual'], width, label='Stock Actual', alpha=0.8)\n",
    "    ax.bar(x + width/2, predicciones_df['stock_predicho'], width, label='Stock Predicho', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Producto ID')\n",
    "    ax.set_ylabel('Stock')\n",
    "    ax.set_title(f'Predicci√≥n de Stock para {fecha_prediccion.strftime(\"%Y-%m-%d\")}')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(predicciones_df['product_id'], rotation=45)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('multi_product_prediction.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
